{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Prediction of Sahelian Summer Rainfall\n",
    "***\n",
    "\n",
    "#### Resources:\n",
    "* [Mardata Course](https://github.com/mardatade/Course-Python-for-Machine-Learning/blob/master/3.%20Neural%20Network.ipynb)\n",
    "* [Keras for Data Scientists](https://keras.io/getting_started/intro_to_keras_for_engineers/#data-loading-amp-preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit, GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1. Data Loading & Preprocessing\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### a) Loading & Normalization\n",
    "\n",
    "**predictor:** contains the data used for the inputs  \n",
    "**label:** from Sahelrainfall data serves as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = xr.open_dataset('data/da_pred_all.nc').to_dataframe()\n",
    "\n",
    "predictor_unit = pd.DataFrame(\n",
    "    data = StandardScaler().fit_transform(predictor), \n",
    "    columns = predictor.columns,\n",
    "    index =  predictor.index\n",
    ")\n",
    "\n",
    "\n",
    "# load validatoin data (Summer Rainfall over Sahel and scale to [cm/month]) \n",
    "labels = np.mean(np.loadtxt(\"data/da_o_sahelprecip19012017.txt\", skiprows=8,)[:,7:10] * 0.01,  axis=1)\n",
    "\n",
    "predictor_unit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### b) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit PCA transformation\n",
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(predictor_unit)\n",
    "\n",
    "\n",
    "# Create Create Pandas DF from PCs\n",
    "col = []\n",
    "for i in range(1, 21):\n",
    "    col.append(f'PC{i}')\n",
    "\n",
    "predictor_pc = pd.DataFrame(\n",
    "    data = principalComponents,\n",
    "    columns = col,\n",
    "    index =  predictor.index\n",
    ")\n",
    "\n",
    "# Test for unit-variance and zero mean:\n",
    "# np.std(pred_pc)\n",
    "# np.mean(pred_pc)\n",
    "# pred_pc.head()\n",
    "\n",
    "predictor_pc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2. MODEL SETUP AND TUNING\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Set Log Parent Directory\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Parent Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = 'logs/tf_no dask/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clear Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -rf logs/tf_nodask/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Hyperparameter Selection\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "#####EXAMPLE SETUP FOR TESTING#####\n",
    "###################################\n",
    "\n",
    "\n",
    "#GRID SERACH HYPERPARAMETER#\n",
    "#---------------------------\n",
    "HP_INPUT_VAR_NINE = hp.HParam('input_var_nine', hp.Discrete(['PC9', 'PC14']),display_name='9th Input Variable')\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['AdamW', 'SGDW']),display_name='Optimizer')\n",
    "HP_LEARN_RATE = hp.HParam('learn_rate', hp.Discrete([0.1]),display_name='Learning Rate')\n",
    "HP_WEIGHT_DECAY = hp.HParam('weight_decay', hp.Discrete([1e-1]),display_name='Weight Decay')\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([1]),display_name='Batch Size')\n",
    "HP_EPOCHS = hp.HParam('n_epochs', hp.Discrete([3]),display_name='Epochs')\n",
    "\n",
    "\n",
    "#CROSS VALIDATION PARAMETER (NO PART OF GRID SEARCH)#\n",
    "#----------------------------------------------------\n",
    "cv_param={\n",
    "    'N_FOLDS': 2,         # number of folds -> small for Test Runs\n",
    "    'TEST_FRAC': .1    # factrion that is held out for test\n",
    "}\n",
    "\n",
    "\n",
    "#BAGGING PARAMETER (NO PART OF GRID SEARCH)#\n",
    "#-------------------------------------------\n",
    "n_baggs = 2  # number of baggs -> small for test runs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "#####FULL SETUP#####\n",
    "####################\n",
    "\n",
    "\n",
    "# #GRID SERACH HYPERPARAMETER#\n",
    "# #---------------------------\n",
    "# HP_INPUT_VAR_NINE = hp.HParam('input_var_nine', hp.Discrete(['PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16']),display_name='9th Input Variable')\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['AdamW', 'SGDW']),display_name='Optimizer')\n",
    "# HP_LEARN_RATE = hp.HParam('learn_rate', hp.Discrete([0.01, 0.1, 0.2]),display_name='Learning Rate')\n",
    "# HP_WEIGHT_DECAY = hp.HParam('weight_decay', hp.Discrete([0.001, 0.01, 0.1]),display_name='Weight Decay')\n",
    "# HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([1, 3, 10, 30]),display_name='Batch Size')\n",
    "# HP_EPOCHS = hp.HParam('n_epochs', hp.Discrete([30, 80, 120]),display_name='Epochs')\n",
    "\n",
    "\n",
    "# #CROSS VALIDATION PARAMETER (NO PART OF GRID SEARCH)#\n",
    "# #----------------------------------------------------\n",
    "# cv_param={\n",
    "#     'N_FOLDS': 105,      # number of folds -> sample size as in Badr\n",
    "#     'TEST_FRAC': .1    # factrion that is held out for test\n",
    "# }\n",
    "\n",
    "\n",
    "# #BAGGING PARAMETER (NO PART OF GRID SEARCH)#\n",
    "# #-------------------------------------------\n",
    "# n_baggs = 10 # number of baggs -> 10 as in Badr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Metric Selection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_TRAIN_MSE_MU= 'train_mse_mu'\n",
    "METRIC_TRAIN_MSE_SIG= 'train_,mse_sig'\n",
    "METRIC_TRAIN_CORR_MU= 'train_corr_mu'\n",
    "METRIC_TRAIN_CORR_SIG= 'train_corr_sig'\n",
    "\n",
    "METRIC_TEST_MSE_MU= 'test_mse_mu'\n",
    "METRIC_TEST_MSE_SIG= 'test_mse_sig'\n",
    "METRIC_TEST_CORR_MU= 'test_corr_mu'\n",
    "METRIC_TEST_CORR_SIG= 'test_corr_sig'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Log Experiment Confiuration to TensorBoard\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.summary.create_file_writer(parent_dir).as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_INPUT_VAR_NINE, HP_OPTIMIZER, HP_LEARN_RATE, HP_WEIGHT_DECAY, HP_BATCH_SIZE, HP_EPOCHS],\n",
    "        metrics=[\n",
    "            hp.Metric(METRIC_TRAIN_MSE_MU, display_name='Training Sample MSE µ'),\n",
    "            hp.Metric(METRIC_TRAIN_MSE_SIG, display_name='Training Sample  MSE σ'),\n",
    "            hp.Metric(METRIC_TRAIN_CORR_MU, display_name='Training Sample Correlation µ'),\n",
    "            hp.Metric(METRIC_TRAIN_CORR_SIG, display_name='Training Sample  Correlation σ'),\n",
    "            hp.Metric(METRIC_TEST_MSE_MU, display_name='Test Sample MSE µ'),\n",
    "            hp.Metric(METRIC_TEST_MSE_SIG, display_name='Test Sample  MSE σ'),\n",
    "            hp.Metric(METRIC_TEST_CORR_MU, display_name='Test Sample Correlation µ'),\n",
    "            hp.Metric(METRIC_TEST_CORR_SIG, display_name='Test Sample  Correlation σ')\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Build Model Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildModel(hparams):      \n",
    "    \n",
    "    \n",
    "    model = keras.Sequential([\n",
    "            layers.Dense(3, activation=\"sigmoid\", name=\"layer1\", input_shape=(9,)),\n",
    "            layers.Dense(1, activation='linear', name='output')\n",
    "        ])\n",
    "    \n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=getattr(tfa.optimizers, hparams[HP_OPTIMIZER])(\n",
    "            learning_rate=hparams[HP_LEARN_RATE],\n",
    "            weight_decay=hparams[HP_WEIGHT_DECAY]\n",
    "        )\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Bagging Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bagging(hparams, features, model, train_index, test_index):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # set emty output matrices\n",
    "    y_train_bagging = np.zeros((train_index.size, n_baggs))\n",
    "    y_test_bagging = np.zeros((test_index.size, n_baggs))    \n",
    "    \n",
    "    \n",
    "    #Train the model 'n_baggs' times and store model predictions into matrice\n",
    "    for n in range(n_baggs):\n",
    "        \n",
    "#         print ('baggin run', n)\n",
    "#         print ('PREDICTION ON TEST DATA:', y_test_bagging)\n",
    "        \n",
    "        # Bootstrap sampling from training Data with Size(Training Data)\n",
    "        train_index_bootstrap = np.random.choice(train_index, train_index.size)\n",
    "\n",
    "        #Train the model \n",
    "        model.fit(\n",
    "            features[train_index_bootstrap],\n",
    "            labels[train_index_bootstrap],\n",
    "            batch_size=hparams[HP_BATCH_SIZE],\n",
    "            epochs=hparams[HP_EPOCHS],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        #Run the model for insample data and store in one matrix:\n",
    "        y_train_bagging[:, n] = np.squeeze(model.predict(features[train_index]))\n",
    "        \n",
    "        # ... and for out of sample data        \n",
    "        y_test_bagging[:, n] = np.squeeze(model.predict(features[test_index]))\n",
    "\n",
    "    #return mean of the outputs over baggins (1st dimension)\n",
    "    return y_train_bagging.mean(1), y_test_bagging.mean(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Cross Validation Training & Error Calculation Funktion\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(hparams, cv_param, predictor_pc, labels):\n",
    "        \n",
    "    \n",
    "    train_mse = np.empty(cv_param['N_FOLDS'])\n",
    "    train_corr = np.empty(cv_param['N_FOLDS'])\n",
    "    \n",
    "    test_mse = np.empty(cv_param['N_FOLDS'])\n",
    "    test_corr = np.empty(cv_param['N_FOLDS'])\n",
    "    \n",
    "    #choose Inputs\n",
    "    features = predictor_pc.loc[:,['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', hparams[HP_INPUT_VAR_NINE]]].to_numpy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Cross Validation#\n",
    "    ##################\n",
    "    \n",
    "    cv_fold = 0\n",
    "    \n",
    "    for train_index, test_index in ShuffleSplit(n_splits=cv_param['N_FOLDS'], test_size=cv_param['TEST_FRAC']).split(features):\n",
    "        \n",
    "#         print(cv_fold)\n",
    "#         print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        \n",
    "    \n",
    "        # Build the model according to definition:\n",
    "        model = BuildModel(hparams)\n",
    "        \n",
    "        # Train and predict using Bagging    \n",
    "        y_train, y_test = Bagging(hparams, features, model, train_index, test_index)\n",
    "        \n",
    "        \n",
    "        #Compute error metrics for in sample data\n",
    "        train_err=  y_train - labels[train_index]\n",
    "        train_mse[cv_fold] = np.mean(train_err**2)\n",
    "        train_corr[cv_fold] = st.pearsonr(y_train, labels[train_index])[0]\n",
    "        \n",
    "        # ... and for out of sample data\n",
    "        test_err=  y_test - labels[test_index]\n",
    "        test_mse[cv_fold] = np.mean(test_err**2)\n",
    "        test_corr[cv_fold] = st.pearsonr(y_test, labels[test_index])[0]\n",
    "        \n",
    "        \n",
    "#         print ( \"BAGGING OUT Test\", y_test)\n",
    "        cv_fold += 1\n",
    "    \n",
    "    #######################################################################################################\n",
    "    \n",
    "    \n",
    "    #Error Moments#\n",
    "    ###############\n",
    "    \n",
    "    eval_metrics = {\n",
    "        'train_mse_mu': np.mean(train_mse),\n",
    "        'train_mse_sig': np.std(train_mse),\n",
    "        'train_corr_mu': np.mean(train_corr),\n",
    "        'train_corr_sig': np.std(train_corr),            \n",
    "        'test_mse_mu': np.mean(test_mse),\n",
    "        'test_mse_sig': np.std(test_mse),\n",
    "        'test_corr_mu': np.mean(test_corr),\n",
    "        'test_corr_sig': np.std(test_corr),\n",
    "    }\n",
    "    \n",
    "#     print(eval_metrics)\n",
    "    \n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Model Run and Log Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunModel(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        \n",
    "        eval_metrics = TrainModel(hparams, cv_param, predictor_pc, labels)\n",
    "        eval_metrics.a()\n",
    "        tf.summary.scalar(METRIC_TRAIN_MSE_MU,   eval_metrics['train_mse_mu'],   step=1)\n",
    "        tf.summary.scalar(METRIC_TRAIN_MSE_SIG,  eval_metrics['train_mse_sig'],  step=1)\n",
    "        tf.summary.scalar(METRIC_TRAIN_CORR_MU,  eval_metrics['train_corr_mu'],  step=1)\n",
    "        tf.summary.scalar(METRIC_TRAIN_CORR_SIG, eval_metrics['train_corr_sig'], step=1)\n",
    "        tf.summary.scalar(METRIC_TEST_MSE_MU,    eval_metrics['test_mse_mu'],    step=1)\n",
    "        tf.summary.scalar(METRIC_TEST_MSE_SIG,   eval_metrics['test_mse_sig'],   step=1)\n",
    "        tf.summary.scalar(METRIC_TEST_CORR_MU,   eval_metrics['test_corr_mu'],   step=1)\n",
    "        tf.summary.scalar(METRIC_TEST_CORR_SIG,  eval_metrics['test_corr_sig'],  step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Grid Search\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0        \n",
    "\n",
    "for input_var_nine in HP_INPUT_VAR_NINE.domain.values:\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "        for learn_rate in HP_LEARN_RATE.domain.values:\n",
    "            for weight_decay in HP_WEIGHT_DECAY.domain.values:\n",
    "                for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "                    for n_epochs in HP_EPOCHS.domain.values:\n",
    "\n",
    "\n",
    "                        hparams = {\n",
    "                            HP_INPUT_VAR_NINE: input_var_nine,\n",
    "                            HP_OPTIMIZER: optimizer,\n",
    "                            HP_LEARN_RATE: learn_rate,\n",
    "                            HP_WEIGHT_DECAY: weight_decay,\n",
    "                            HP_BATCH_SIZE: batch_size,\n",
    "                            HP_EPOCHS: n_epochs,                \n",
    "                        }\n",
    "\n",
    "                        run_name = f\"run-{session_num:04d}\"\n",
    "#                         print(f'--- Starting trial: {run_name}')\n",
    "#                         print({h.name: hparams[h] for h in hparams})\n",
    "                        RunModel(parent_dir + run_name, hparams)\n",
    "                        session_num += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "***\n",
    "***\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
