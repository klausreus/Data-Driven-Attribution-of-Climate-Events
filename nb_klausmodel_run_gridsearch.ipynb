{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Grid Search for Neural Network Prediction of Sahelian S\n",
    "***\n",
    "\n",
    "#### Resources:\n",
    "* [Mardata Course](https://github.com/mardatade/Course-Python-for-Machine-Learning/blob/master/3.%20Neural%20Network.ipynb)\n",
    "* [Keras for Data Scientists](https://keras.io/getting_started/intro_to_keras_for_engineers/#data-loading-amp-preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import xarray as xr\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit, GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import TensorBoard \n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "%load_ext tensorboard\n",
    "\n",
    "import dask\n",
    "from dask import delayed\n",
    "import dask.bag as db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:34479</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>135.09 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:34479' processes=4 threads=16, memory=135.09 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://127.0.0.1:34479\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask.config.set(scheduler='synchronous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1. Data Loading & Preprocessing\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### a) Loading & Normalization\n",
    "\n",
    "**predictor:** contains the data used for the inputs  \n",
    "**label:** from Sahelrainfall data serves as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>siod_e</th>\n",
       "      <th>siod_w</th>\n",
       "      <th>sst_med</th>\n",
       "      <th>tsa</th>\n",
       "      <th>tna</th>\n",
       "      <th>sst_mdr</th>\n",
       "      <th>sata_lnh</th>\n",
       "      <th>sata_lsh</th>\n",
       "      <th>sata_onh</th>\n",
       "      <th>sata_osh</th>\n",
       "      <th>slp_darwin</th>\n",
       "      <th>slp_tahiti</th>\n",
       "      <th>amo</th>\n",
       "      <th>nao</th>\n",
       "      <th>pdo</th>\n",
       "      <th>np</th>\n",
       "      <th>nino12</th>\n",
       "      <th>nino3</th>\n",
       "      <th>nino34</th>\n",
       "      <th>nino4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>-1.100027</td>\n",
       "      <td>-1.152764</td>\n",
       "      <td>-0.745530</td>\n",
       "      <td>-0.595366</td>\n",
       "      <td>0.388372</td>\n",
       "      <td>0.608415</td>\n",
       "      <td>-0.123443</td>\n",
       "      <td>-0.732091</td>\n",
       "      <td>-0.497808</td>\n",
       "      <td>-0.737797</td>\n",
       "      <td>0.074807</td>\n",
       "      <td>1.634819</td>\n",
       "      <td>0.923204</td>\n",
       "      <td>0.917456</td>\n",
       "      <td>-0.193321</td>\n",
       "      <td>1.938388</td>\n",
       "      <td>-0.950168</td>\n",
       "      <td>-0.595561</td>\n",
       "      <td>-0.214314</td>\n",
       "      <td>-0.079270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>0.088643</td>\n",
       "      <td>0.340415</td>\n",
       "      <td>-1.507314</td>\n",
       "      <td>-0.954566</td>\n",
       "      <td>-0.346586</td>\n",
       "      <td>-0.173588</td>\n",
       "      <td>-1.289978</td>\n",
       "      <td>-0.201810</td>\n",
       "      <td>-1.175314</td>\n",
       "      <td>-0.987096</td>\n",
       "      <td>1.443896</td>\n",
       "      <td>2.682485</td>\n",
       "      <td>-0.620146</td>\n",
       "      <td>-1.172590</td>\n",
       "      <td>0.819716</td>\n",
       "      <td>-0.162154</td>\n",
       "      <td>0.991321</td>\n",
       "      <td>0.969845</td>\n",
       "      <td>1.099218</td>\n",
       "      <td>1.070532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>-0.900789</td>\n",
       "      <td>0.669332</td>\n",
       "      <td>-2.243639</td>\n",
       "      <td>-2.186294</td>\n",
       "      <td>-0.101970</td>\n",
       "      <td>0.283583</td>\n",
       "      <td>-1.333183</td>\n",
       "      <td>-1.076056</td>\n",
       "      <td>-1.415719</td>\n",
       "      <td>-1.333946</td>\n",
       "      <td>-0.071881</td>\n",
       "      <td>1.535042</td>\n",
       "      <td>-0.458290</td>\n",
       "      <td>-1.030410</td>\n",
       "      <td>-0.186187</td>\n",
       "      <td>0.530864</td>\n",
       "      <td>-0.371251</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.524139</td>\n",
       "      <td>0.842095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>-0.949568</td>\n",
       "      <td>-1.056219</td>\n",
       "      <td>-0.079925</td>\n",
       "      <td>-1.975498</td>\n",
       "      <td>-2.214111</td>\n",
       "      <td>-1.894743</td>\n",
       "      <td>-1.135674</td>\n",
       "      <td>-1.133384</td>\n",
       "      <td>-1.863746</td>\n",
       "      <td>-1.778347</td>\n",
       "      <td>-0.903114</td>\n",
       "      <td>1.235708</td>\n",
       "      <td>-1.872482</td>\n",
       "      <td>1.447076</td>\n",
       "      <td>-0.892459</td>\n",
       "      <td>0.756497</td>\n",
       "      <td>-0.307712</td>\n",
       "      <td>-0.234313</td>\n",
       "      <td>-0.475713</td>\n",
       "      <td>-0.741738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>-0.034350</td>\n",
       "      <td>-0.632249</td>\n",
       "      <td>-0.718895</td>\n",
       "      <td>-1.684676</td>\n",
       "      <td>-1.334312</td>\n",
       "      <td>-1.014906</td>\n",
       "      <td>-1.314666</td>\n",
       "      <td>-0.595938</td>\n",
       "      <td>-1.284589</td>\n",
       "      <td>-0.954579</td>\n",
       "      <td>0.759351</td>\n",
       "      <td>-2.655622</td>\n",
       "      <td>-0.499163</td>\n",
       "      <td>-1.289888</td>\n",
       "      <td>0.545055</td>\n",
       "      <td>-0.326007</td>\n",
       "      <td>1.227830</td>\n",
       "      <td>1.497381</td>\n",
       "      <td>1.439037</td>\n",
       "      <td>1.032459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        siod_e    siod_w   sst_med       tsa       tna   sst_mdr  sata_lnh  \\\n",
       "year                                                                         \n",
       "1901 -1.100027 -1.152764 -0.745530 -0.595366  0.388372  0.608415 -0.123443   \n",
       "1902  0.088643  0.340415 -1.507314 -0.954566 -0.346586 -0.173588 -1.289978   \n",
       "1903 -0.900789  0.669332 -2.243639 -2.186294 -0.101970  0.283583 -1.333183   \n",
       "1904 -0.949568 -1.056219 -0.079925 -1.975498 -2.214111 -1.894743 -1.135674   \n",
       "1905 -0.034350 -0.632249 -0.718895 -1.684676 -1.334312 -1.014906 -1.314666   \n",
       "\n",
       "      sata_lsh  sata_onh  sata_osh  slp_darwin  slp_tahiti       amo  \\\n",
       "year                                                                   \n",
       "1901 -0.732091 -0.497808 -0.737797    0.074807    1.634819  0.923204   \n",
       "1902 -0.201810 -1.175314 -0.987096    1.443896    2.682485 -0.620146   \n",
       "1903 -1.076056 -1.415719 -1.333946   -0.071881    1.535042 -0.458290   \n",
       "1904 -1.133384 -1.863746 -1.778347   -0.903114    1.235708 -1.872482   \n",
       "1905 -0.595938 -1.284589 -0.954579    0.759351   -2.655622 -0.499163   \n",
       "\n",
       "           nao       pdo        np    nino12     nino3    nino34     nino4  \n",
       "year                                                                        \n",
       "1901  0.917456 -0.193321  1.938388 -0.950168 -0.595561 -0.214314 -0.079270  \n",
       "1902 -1.172590  0.819716 -0.162154  0.991321  0.969845  1.099218  1.070532  \n",
       "1903 -1.030410 -0.186187  0.530864 -0.371251  0.000784  0.524139  0.842095  \n",
       "1904  1.447076 -0.892459  0.756497 -0.307712 -0.234313 -0.475713 -0.741738  \n",
       "1905 -1.289888  0.545055 -0.326007  1.227830  1.497381  1.439037  1.032459  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = xr.open_dataset('data/da_pred_all.nc').to_dataframe()\n",
    "\n",
    "predictor_unit = pd.DataFrame(\n",
    "    data = StandardScaler().fit_transform(predictor), \n",
    "    columns = predictor.columns,\n",
    "    index =  predictor.index\n",
    ")\n",
    "\n",
    "\n",
    "# load validatoin data (Summer Rainfall over Sahel and scale to [cm/month]) \n",
    "labels = np.mean(np.loadtxt(\"data/da_o_sahelprecip19012017.txt\", skiprows=8,)[:,7:10] * 0.01,  axis=1)\n",
    "\n",
    "predictor_unit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### b) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>PC11</th>\n",
       "      <th>PC12</th>\n",
       "      <th>PC13</th>\n",
       "      <th>PC14</th>\n",
       "      <th>PC15</th>\n",
       "      <th>PC16</th>\n",
       "      <th>PC17</th>\n",
       "      <th>PC18</th>\n",
       "      <th>PC19</th>\n",
       "      <th>PC20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>-1.568434</td>\n",
       "      <td>-0.814406</td>\n",
       "      <td>-1.565711</td>\n",
       "      <td>-1.237071</td>\n",
       "      <td>1.472947</td>\n",
       "      <td>-0.593357</td>\n",
       "      <td>-1.660153</td>\n",
       "      <td>0.222967</td>\n",
       "      <td>0.241259</td>\n",
       "      <td>0.837099</td>\n",
       "      <td>0.098283</td>\n",
       "      <td>-0.767361</td>\n",
       "      <td>-0.099698</td>\n",
       "      <td>0.875102</td>\n",
       "      <td>-0.391004</td>\n",
       "      <td>0.067938</td>\n",
       "      <td>-0.012741</td>\n",
       "      <td>0.176348</td>\n",
       "      <td>0.188827</td>\n",
       "      <td>0.012466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>-0.521670</td>\n",
       "      <td>2.782115</td>\n",
       "      <td>-0.365347</td>\n",
       "      <td>2.152565</td>\n",
       "      <td>2.364542</td>\n",
       "      <td>0.248512</td>\n",
       "      <td>-1.855028</td>\n",
       "      <td>-0.231364</td>\n",
       "      <td>0.177858</td>\n",
       "      <td>0.788062</td>\n",
       "      <td>-0.135515</td>\n",
       "      <td>0.526785</td>\n",
       "      <td>-0.439029</td>\n",
       "      <td>0.236945</td>\n",
       "      <td>0.613192</td>\n",
       "      <td>-0.173206</td>\n",
       "      <td>-0.049516</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.020811</td>\n",
       "      <td>-0.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>-2.357779</td>\n",
       "      <td>1.783520</td>\n",
       "      <td>-1.194862</td>\n",
       "      <td>1.368700</td>\n",
       "      <td>1.273662</td>\n",
       "      <td>-1.280782</td>\n",
       "      <td>-1.766921</td>\n",
       "      <td>0.441110</td>\n",
       "      <td>-0.412690</td>\n",
       "      <td>0.913848</td>\n",
       "      <td>-1.644854</td>\n",
       "      <td>0.649893</td>\n",
       "      <td>-0.240188</td>\n",
       "      <td>0.273618</td>\n",
       "      <td>0.151983</td>\n",
       "      <td>0.152255</td>\n",
       "      <td>-0.063746</td>\n",
       "      <td>0.036216</td>\n",
       "      <td>0.128500</td>\n",
       "      <td>0.007985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>-5.168501</td>\n",
       "      <td>0.642722</td>\n",
       "      <td>1.054827</td>\n",
       "      <td>-0.759804</td>\n",
       "      <td>1.351015</td>\n",
       "      <td>1.020594</td>\n",
       "      <td>-0.413493</td>\n",
       "      <td>-0.695127</td>\n",
       "      <td>-0.606953</td>\n",
       "      <td>-0.289135</td>\n",
       "      <td>-0.644362</td>\n",
       "      <td>0.201273</td>\n",
       "      <td>-0.556953</td>\n",
       "      <td>0.270713</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>-0.383464</td>\n",
       "      <td>0.148037</td>\n",
       "      <td>-0.071637</td>\n",
       "      <td>0.241812</td>\n",
       "      <td>0.073306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>-1.282539</td>\n",
       "      <td>4.379097</td>\n",
       "      <td>0.640607</td>\n",
       "      <td>0.433165</td>\n",
       "      <td>-0.940971</td>\n",
       "      <td>-1.433710</td>\n",
       "      <td>1.068731</td>\n",
       "      <td>-1.191238</td>\n",
       "      <td>0.581742</td>\n",
       "      <td>-0.393126</td>\n",
       "      <td>-0.344745</td>\n",
       "      <td>-0.274467</td>\n",
       "      <td>0.226232</td>\n",
       "      <td>-0.207231</td>\n",
       "      <td>0.295327</td>\n",
       "      <td>-0.322379</td>\n",
       "      <td>0.324228</td>\n",
       "      <td>-0.052399</td>\n",
       "      <td>0.166036</td>\n",
       "      <td>0.116238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
       "year                                                                         \n",
       "1901 -1.568434 -0.814406 -1.565711 -1.237071  1.472947 -0.593357 -1.660153   \n",
       "1902 -0.521670  2.782115 -0.365347  2.152565  2.364542  0.248512 -1.855028   \n",
       "1903 -2.357779  1.783520 -1.194862  1.368700  1.273662 -1.280782 -1.766921   \n",
       "1904 -5.168501  0.642722  1.054827 -0.759804  1.351015  1.020594 -0.413493   \n",
       "1905 -1.282539  4.379097  0.640607  0.433165 -0.940971 -1.433710  1.068731   \n",
       "\n",
       "           PC8       PC9      PC10      PC11      PC12      PC13      PC14  \\\n",
       "year                                                                         \n",
       "1901  0.222967  0.241259  0.837099  0.098283 -0.767361 -0.099698  0.875102   \n",
       "1902 -0.231364  0.177858  0.788062 -0.135515  0.526785 -0.439029  0.236945   \n",
       "1903  0.441110 -0.412690  0.913848 -1.644854  0.649893 -0.240188  0.273618   \n",
       "1904 -0.695127 -0.606953 -0.289135 -0.644362  0.201273 -0.556953  0.270713   \n",
       "1905 -1.191238  0.581742 -0.393126 -0.344745 -0.274467  0.226232 -0.207231   \n",
       "\n",
       "          PC15      PC16      PC17      PC18      PC19      PC20  \n",
       "year                                                              \n",
       "1901 -0.391004  0.067938 -0.012741  0.176348  0.188827  0.012466  \n",
       "1902  0.613192 -0.173206 -0.049516  0.003676  0.020811 -0.022700  \n",
       "1903  0.151983  0.152255 -0.063746  0.036216  0.128500  0.007985  \n",
       "1904  0.003502 -0.383464  0.148037 -0.071637  0.241812  0.073306  \n",
       "1905  0.295327 -0.322379  0.324228 -0.052399  0.166036  0.116238  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit PCA transformation\n",
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(predictor_unit)\n",
    "\n",
    "\n",
    "# Create Create Pandas DF from PCs\n",
    "col = []\n",
    "for i in range(1, 21):\n",
    "    col.append(f'PC{i}')\n",
    "\n",
    "predictor_pc = pd.DataFrame(\n",
    "    data = principalComponents,\n",
    "    columns = col,\n",
    "    index =  predictor.index\n",
    ")\n",
    "\n",
    "# Test for unit-variance and zero mean:\n",
    "# np.std(pred_pc)\n",
    "# np.mean(pred_pc)\n",
    "# pred_pc.head()\n",
    "\n",
    "predictor_pc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2. MODEL SETUP AND TUNING\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Set Logs\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Parent Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = 'logs/testrun/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clear Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf logs/testrun/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Hyperparameter Selection\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "#####EXAMPLE SETUP FOR TESTING#####\n",
    "###################################\n",
    "\n",
    "\n",
    "#GRID SERACH HYPERPARAMETER#\n",
    "#---------------------------\n",
    "HP_INPUT_VAR_NINE = hp.HParam('input_var_nine', hp.Discrete(['PC9']),display_name='9th Input Variable')\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['AdamW']),display_name='Optimizer')\n",
    "HP_LEARN_RATE = hp.HParam('learn_rate', hp.Discrete([0.09, 0.1]),display_name='Learning Rate')\n",
    "HP_WEIGHT_DECAY = hp.HParam('weight_decay', hp.Discrete([0.001]),display_name='Weight Decay')\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([1]),display_name='Batch Size')\n",
    "HP_EPOCHS = hp.HParam('n_epochs', hp.Discrete([80]),display_name='Epochs')\n",
    "\n",
    "\n",
    "#CROSS VALIDATION PARAMETER (NO PART OF GRID SEARCH)#\n",
    "#----------------------------------------------------\n",
    "cv_param={\n",
    "    'N_FOLDS': 10,         # number of folds -> small for Test Runs\n",
    "    'TEST_FRAC': .1    # factrion that is held out for test\n",
    "}\n",
    "\n",
    "\n",
    "#BAGGING PARAMETER (NO PART OF GRID SEARCH)#\n",
    "#-------------------------------------------\n",
    "n_baggs = 5  # number of baggs -> small for test runs\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# hist_dir = parent_dir + f\"/run-{HPARAM['grid_num']:04d}\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = keras.callbacks.TensorBoard(log_dir=hist_dir)\n",
    "\n",
    "\n",
    "####################\n",
    "#####FULL SETUP#####\n",
    "####################\n",
    "\n",
    "\n",
    "# #GRID SERACH HYPERPARAMETER#\n",
    "# #---------------------------\n",
    "# HP_INPUT_VAR_NINE = hp.HParam('input_var_nine', hp.Discrete(['PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16']),display_name='9th Input Variable')\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['AdamW', 'SGDW']),display_name='Optimizer')\n",
    "# HP_LEARN_RATE = hp.HParam('learn_rate', hp.Discrete([0.01, 0.1, 0.2]),display_name='Learning Rate')\n",
    "# HP_WEIGHT_DECAY = hp.HParam('weight_decay', hp.Discrete([0.001, 0.01, 0.1]),display_name='Weight Decay')\n",
    "# HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([1, 3, 10, 30]),display_name='Batch Size')\n",
    "# HP_EPOCHS = hp.HParam('n_epochs', hp.Discrete([30, 80, 120]),display_name='Epochs')\n",
    "\n",
    "\n",
    "# #CROSS VALIDATION PARAMETER (NO PART OF GRID SEARCH)#\n",
    "# #----------------------------------------------------\n",
    "# cv_param={\n",
    "#     'N_FOLDS': 105,      # number of folds -> sample size as in Badr\n",
    "#     'TEST_FRAC': .1    # factrion that is held out for test\n",
    "# }\n",
    "\n",
    "\n",
    "# #BAGGING PARAMETER (NO PART OF GRID SEARCH)#\n",
    "# #-------------------------------------------\n",
    "# n_baggs = 10 # number of baggs -> 10 as in Badr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_Dropout = hp.HParam('dropout', hp.RealInterval(0.0, 1.0),display_name='Dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i in (HP_Dropout.domain.min_value , HP_Dropout.domain.max_value):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        property\n",
       "\u001b[0;31mString form:\u001b[0m <property object at 0x7f8ef5a574f0>\n",
       "\u001b[0;31mDocstring:\u001b[0m   <no docstring>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hp.HParam.domain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Discrete',\n",
       " 'Domain',\n",
       " 'HParam',\n",
       " 'IntInterval',\n",
       " 'KerasCallback',\n",
       " 'Metric',\n",
       " 'RealInterval',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'hparams',\n",
       " 'hparams_config',\n",
       " 'hparams_config_pb',\n",
       " 'hparams_pb']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Metric Selection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_TRAIN_MSE_MU= 'train_mse_mu'\n",
    "METRIC_TRAIN_MSE_SIG= 'train_mse_sig'\n",
    "METRIC_TRAIN_CORR_MU= 'train_corr_mu'\n",
    "METRIC_TRAIN_CORR_SIG= 'train_corr_sig'\n",
    "\n",
    "METRIC_TEST_MSE_MU= 'test_mse_mu'\n",
    "METRIC_TEST_MSE_SIG= 'test_mse_sig'\n",
    "METRIC_TEST_CORR_MU= 'test_corr_mu'\n",
    "METRIC_TEST_CORR_SIG= 'test_corr_sig'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Log Experiment Confiuration to TensorBoard\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.summary.create_file_writer(parent_dir).as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_INPUT_VAR_NINE, HP_OPTIMIZER, HP_LEARN_RATE, HP_WEIGHT_DECAY, HP_BATCH_SIZE, HP_EPOCHS],\n",
    "        metrics=[\n",
    "            hp.Metric(METRIC_TRAIN_MSE_MU, display_name='Training Sample MSE µ'),\n",
    "            hp.Metric(METRIC_TRAIN_MSE_SIG, display_name='Training Sample  MSE σ'),\n",
    "            hp.Metric(METRIC_TRAIN_CORR_MU, display_name='Training Sample Correlation µ'),\n",
    "            hp.Metric(METRIC_TRAIN_CORR_SIG, display_name='Training Sample  Correlation σ'),\n",
    "            hp.Metric(METRIC_TEST_MSE_MU, display_name='Test Sample MSE µ'),\n",
    "            hp.Metric(METRIC_TEST_MSE_SIG, display_name='Test Sample  MSE σ'),\n",
    "            hp.Metric(METRIC_TEST_CORR_MU, display_name='Test Sample Correlation µ'),\n",
    "            hp.Metric(METRIC_TEST_CORR_SIG, display_name='Test Sample  Correlation σ')\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Build Model Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildModel(HPARAM):      \n",
    "    \n",
    "    \n",
    "    model = keras.Sequential([\n",
    "            layers.Dense(3, activation=\"sigmoid\", name=\"layer1\", input_shape=(9,)),\n",
    "            layers.Dense(3, activation=\"sigmoid\", name=\"layer2\"),\n",
    "            layers.Dense(3, activation=\"sigmoid\", name=\"layer3\"),\n",
    "            layers.Dense(1, activation='linear', name='output')\n",
    "        ])\n",
    "    \n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=getattr(tfa.optimizers, HPARAM['optimizer'])(\n",
    "            learning_rate=HPARAM['learn_rate'],\n",
    "            weight_decay=HPARAM['weight_decay']\n",
    "        )\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 3)                 30        \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 58\n",
      "Trainable params: 58\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "testmodel = BuildModel({'optimizer': 'AdamW', 'learn_rate': 0.1, 'weight_decay': 0.01})\n",
    "print(testmodel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Single Run Training & Error Calculation Funktion\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hparams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-65b6401daf92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"/run-{hparams[0]['grid_num']:04d}\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"/split-{SPLIT['split_num']:02d}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhist_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hparams' is not defined"
     ]
    }
   ],
   "source": [
    "hist_dir = parent_dir + f\"/run-{hparams[0]['grid_num']:04d}\" + f\"/split-{SPLIT['split_num']:02d}\"\n",
    "hist_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(SPLIT, HPARAM, features):\n",
    "    \n",
    "    model = BuildModel(HPARAM)\n",
    "    \n",
    "    hist_dir = parent_dir + f\"/run-{HPARAM['grid_num']:04d}\" + f\"/split-{SPLIT['split_num']:02d}\"\n",
    "    \n",
    "    train_history = model.fit(\n",
    "        features[SPLIT['train_index']],\n",
    "        labels[SPLIT['train_index']],\n",
    "        batch_size=HPARAM['batch_size'],\n",
    "        epochs=HPARAM['n_epochs'],\n",
    "        verbose=0,\n",
    "#         callbacks=[earlystop],\n",
    "    )\n",
    "    \n",
    "\n",
    "    y_train = np.squeeze(model.predict(features[SPLIT['train_index']]))\n",
    "    y_test = np.squeeze(model.predict(features[SPLIT['test_index']]))\n",
    "\n",
    "    \n",
    "    train_error = y_train - labels[SPLIT['train_index']]\n",
    "    train_mse = np.mean(train_error**2)\n",
    "    train_corr = st.pearsonr(y_train, labels[SPLIT['train_index']])[0]\n",
    "    \n",
    "    test_error = y_test - labels[SPLIT['test_index']]\n",
    "    test_mse = np.mean(test_error**2)\n",
    "    test_corr = st.pearsonr(y_test, labels[SPLIT['test_index']])[0]\n",
    "\n",
    "    \n",
    "    metrics = {\n",
    "        'train_mse': train_mse,\n",
    "        'train_corr': train_corr,\n",
    "        'test_mse': test_mse,\n",
    "        'test_corr': test_corr,\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Cross-Validation and Log Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TuneModel(HPARAM):\n",
    "    \n",
    "    \n",
    "    with tf.summary.create_file_writer(parent_dir + f\"/run-{HPARAM['grid_num']:04d}\").as_default():\n",
    "        hp.hparams({\n",
    "            HP_INPUT_VAR_NINE: HPARAM['input_var_nine'],\n",
    "            HP_OPTIMIZER: HPARAM['optimizer'],\n",
    "            HP_LEARN_RATE: HPARAM['learn_rate'],\n",
    "            HP_WEIGHT_DECAY: HPARAM['weight_decay'],\n",
    "            HP_BATCH_SIZE: HPARAM['batch_size'],\n",
    "            HP_EPOCHS: HPARAM['n_epochs']\n",
    "        })\n",
    "        \n",
    "        features = predictor_pc.loc[:,['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', HPARAM['input_var_nine']]].to_numpy()\n",
    "        \n",
    "        metrics = SPLITS.map(lambda SPLIT: TrainModel(SPLIT, HPARAM, features)).compute()\n",
    "\n",
    "        train_mse = [metric['train_mse'] for metric in metrics]\n",
    "        train_corr = [metric['train_corr'] for metric in metrics]\n",
    "        test_mse = [metric['test_mse'] for metric in metrics]\n",
    "        test_corr = [metric['test_corr'] for metric in metrics]\n",
    "\n",
    "        \n",
    "        tf.summary.scalar(METRIC_TRAIN_MSE_MU,   np.mean(train_mse),  step=1)\n",
    "        tf.summary.scalar(METRIC_TRAIN_MSE_SIG,  np.std(train_mse),   step=1)\n",
    "        tf.summary.scalar(METRIC_TRAIN_CORR_MU,  np.mean(train_corr), step=1)\n",
    "        tf.summary.scalar(METRIC_TRAIN_CORR_SIG, np.std(train_corr),  step=1)\n",
    "        tf.summary.scalar(METRIC_TEST_MSE_MU,    np.mean(test_mse),   step=1)\n",
    "        tf.summary.scalar(METRIC_TEST_MSE_SIG,   np.std(test_mse),    step=1)\n",
    "        tf.summary.scalar(METRIC_TEST_CORR_MU,   np.mean(test_corr),  step=1)\n",
    "        tf.summary.scalar(METRIC_TEST_CORR_SIG,  np.std(test_corr),   step=1)\n",
    "        \n",
    "#     return test_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "###  Perform Grid Search\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create HP Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_num = 0\n",
    "hparams = []\n",
    "for input_var_nine in HP_INPUT_VAR_NINE.domain.values:\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "        for learn_rate in HP_LEARN_RATE.domain.values:\n",
    "            for weight_decay in HP_WEIGHT_DECAY.domain.values:\n",
    "                for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "                    for n_epochs in HP_EPOCHS.domain.values:\n",
    "\n",
    "\n",
    "                        hparams.append(\n",
    "                                {\n",
    "                                'input_var_nine': input_var_nine,\n",
    "                                'optimizer': optimizer,\n",
    "                                'learn_rate': learn_rate,\n",
    "                                'weight_decay': weight_decay,\n",
    "                                'batch_size': batch_size,\n",
    "                                'n_epochs': n_epochs,       \n",
    "                                'grid_num': grid_num\n",
    "                                }\n",
    "                            )\n",
    "                        grid_num += 1\n",
    "                        \n",
    "HPARAMS = db.from_sequence(hparams)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_var_nine': 'PC9',\n",
       "  'optimizer': 'AdamW',\n",
       "  'learn_rate': 0.01,\n",
       "  'weight_decay': 0.001,\n",
       "  'batch_size': 1,\n",
       "  'n_epochs': 80,\n",
       "  'grid_num': 0},)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HPARAMS.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Data Splits Bag (RRHCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_num = 0\n",
    "splits = []\n",
    "for train, test in ShuffleSplit(n_splits=cv_param['N_FOLDS'], test_size=cv_param['TEST_FRAC']).split(predictor_pc):\n",
    "    splits.append(\n",
    "        {\n",
    "        'train_index': train,\n",
    "        'test_index': test,\n",
    "        'split_num': split_num\n",
    "        }\n",
    "    )\n",
    "    split_num += 1 \n",
    "SPLITS = db.from_sequence (splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'train_index': array([ 18,  53, 100,  80, 115, 101,  75,  64,  63,  43,  35, 107,  90,\n",
       "         110,  12, 104,  95,  56,  36,  93,  48,  14,  39,  58,  54, 111,\n",
       "          77,  44,   5,  46,  82, 108,  29,   4,  41,  72,  20,  89,  25,\n",
       "         116,   3,  37,  86,  73,  52,  65,  47,  59,  19, 114,  94,   1,\n",
       "          96,  87,  34,  51,  33,  30,  79,  69,  38,  49,   9,  83,  13,\n",
       "          22,  67,  32,  11,  61,  78,  57,  27, 113,  24,  50,  15,  92,\n",
       "          28,  68,  91,  74,  26,  62,  70,   2,  31,  60,  23,   6,  97,\n",
       "          16, 103,  98,  81, 112,  21,  85,  88,  55,  45,  40,  99,  42,\n",
       "          66]),\n",
       "  'test_index': array([  0,  17,  84,   8,  76,  71, 105,  10, 106,   7, 102, 109]),\n",
       "  'split_num': 0},\n",
       " {'train_index': array([  5, 104,  51,  18, 106,  31, 103,  20,  35,  63,  73,  70,  57,\n",
       "          71,  68, 100,  81,  76, 102,  24,  11,  54,  86,  25,  94,  33,\n",
       "          98,  59,   9,   1,  16, 110,  74,  95,  41,  38,  78, 114,  10,\n",
       "         115,   3,  48, 113,  26,  27, 111,  99,  13,   8,  37,  84,  72,\n",
       "          30,  79,  39,  12,  45,  46,  19,  80,  93, 109,  75,  29, 101,\n",
       "          97,  53,  43,   4,  55,  14,  87,  61,  49,  23,  90,  69,  77,\n",
       "          85,  92,  62,  82,  36, 112,  47,  88,  65,  32,  64, 105,  91,\n",
       "          66,  17,   0,  15,  89,   7,   6, 108,  60,  42,  34,  44,  52,\n",
       "         107]),\n",
       "  'test_index': array([ 58,  50,  96,  21,  56,  28,  67, 116,  40,  83,  22,   2]),\n",
       "  'split_num': 1})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLITS.take(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.7 ms, sys: 4.94 ms, total: 54.6 ms\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = HPARAMS.map(lambda HPARAM: TuneModel(HPARAM)).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pidof` not found.\n"
     ]
    }
   ],
   "source": [
    "%pidof tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 3072859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3072859), started 1:44:58 ago. (Use '!kill 3072859' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3b6bbc37ea991681\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3b6bbc37ea991681\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/testrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Close Client after finishing / before using it in another Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "***\n",
    "***\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Bagging Function (Not used curretnly)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Bagging(hparams, features, model, train_index, test_index):\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # set emty output matrices\n",
    "#     y_train_bagging = np.zeros((train_index.size, n_baggs))\n",
    "#     y_test_bagging = np.zeros((test_index.size, n_baggs))    \n",
    "    \n",
    "    \n",
    "#     #Train the model 'n_baggs' times and store model predictions into matrice\n",
    "#     for n in range(n_baggs):\n",
    "        \n",
    "# #         print ('baggin run', n)\n",
    "# #         print ('PREDICTION ON TEST DATA:', y_test_bagging)\n",
    "        \n",
    "#         # Bootstrap sampling from training Data with Size(Training Data)\n",
    "#         train_index_bootstrap = np.random.choice(train_index, train_index.size)\n",
    "\n",
    "#         #Train the model \n",
    "#         model.fit(\n",
    "#             features[train_index_bootstrap],\n",
    "#             labels[train_index_bootstrap],\n",
    "#             batch_size=hparams[HP_BATCH_SIZE],\n",
    "#             epochs=hparams[HP_EPOCHS],\n",
    "#             verbose=0\n",
    "#         )\n",
    "        \n",
    "#         #Run the model for insample data and store in one matrix:\n",
    "#         y_train_bagging[:, n] = np.squeeze(model.predict(features[train_index]))\n",
    "        \n",
    "#         # ... and for out of sample data        \n",
    "#         y_test_bagging[:, n] = np.squeeze(model.predict(features[test_index]))\n",
    "\n",
    "#     # return mean of the outputs over baggins (1st dimension)\n",
    "#     return y_train_bagging.mean(1), y_test_bagging.mean(1)\n",
    "# #     return y_test_bagging.mean(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
